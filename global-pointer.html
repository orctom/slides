<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Global Pointer</title>

	<link rel="stylesheet" href="static/reveal.css">
	<link rel="stylesheet" href="static/theme/black.css">
	<link rel="stylesheet" href="plugin/chalkboard/style.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<style>
		line {
			stroke: white;
		}

		marker {
			stroke: white;
			fill: white;
		}

		.signal text {
			fill: white;
		}

		.signal text:hover {
			fill: white;
		}

		.actor rect,
		.actor path {
			fill: white;
		}

		.note rect,
		.note path {
			fill: white;
		}

		.align-left {
			text-align: left;
		}
		.report {
			font-size: 0.5em;
		}
		.code-wrapper {
			width: 105% !important;
		}
		.code-wrapper code {
			max-height: 570px !important;
		}
	</style>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-markdown data-background="https://s1.ax1x.com/2022/04/08/L9G8k8.png" data-background-opacity="0.3" data-background-size="contain">
				<textarea data-template>
					# Global Pointer
					## 用统一的方式处理嵌套和非嵌套NER

					https://spaces.ac.cn/archives/8373
				</textarea>
			</section>
			<section data-markdown>
				<textarea data-template>
					## 基本思路

					<p class="align-left">已知有:</p>

					* 长度为$n$的待识别文本序列
					* 其中有$k$个真实实体
					* 总共有$m$个实体类型

					<p class="fragment align-left">该序列有$\frac{n(n+1)}2$个"候选实体"</p>
					<p class="fragment align-left">从这$\frac{n(n+1)}2$个"候选实体"里边挑出真实实体</p>
					<p class="fragment align-left">$\Rightarrow$ $m$个"$\frac{n(n+1)}2$选$k$"的多标签分类问题</p>
				</textarea>
			</section>
			<section data-markdown>
				<script type="text/template">
					## 数学形式

					向量序列 $\left[h_1,h_2,⋯,h_n\right]$, 对于实体$\alpha$
					<p class="fragment">$q_{i,\alpha}=W_{q,\alpha}h_i+b_{q,\alpha} \Longrightarrow [q_{1,\alpha},q_{2,\alpha},\cdots,q_{n,\alpha}]$</p>
					<p class="fragment">$k_{i,\alpha}=W_{k,\alpha}h_i+b_{k,\alpha} \Longrightarrow [k_{1,\alpha},k_{2,\alpha},\cdots,k_{n,\alpha}]$</p>
					<p class="fragment">$s_{\alpha}(i,j) = q_{i,\alpha}^{\top}k_{j,\alpha}$</p>
					<p class="fragment">【用$q_{i,\alpha}$与$k_{i,\alpha}$的内积, 作为片段$t[i:j]$是类型为$\alpha$的实体的打分 (logits)】</p>
				</script>
			</section>
			<section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 相对位置信息对NER很重要

						* 相对距离（考虑不同长短的句子）
						* 方向（字词出现的前后顺序）
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 相对位置信息对NER很重要

						![](https://pic4.zhimg.com/80/v2-640b49e6c36a8fa577a32defd7a16cab_720w.jpg)

						$$PE_t^{\top} PE_{t+k} = PE_t^{\top} PE_{t-k}$$

						TENER https://arxiv.org/abs/1911.04474
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						## 位置编码方式

						* 绝对位置编码
						  - 三角式
						  - 学习式
						  - ...
						* 相对位置编码
						  - XLNet式
						  - 复数式
						  - ...

						https://spaces.ac.cn/archives/8130
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 旋转位置编码

						Rotary Position Embedding (RoPE)

						<p class="fragment">构造一个位置编码矩阵$\mathcal{R_i}$，满足关系$\mathcal{R_i^{\top}} \mathcal{R_j} = \mathcal{R_{j-i}}$</p>
						<p class="fragment">代入$s_{\alpha}(i,j) = q_{i,\alpha}^{\top}k_{j,\alpha}$</p>
						<p class="fragment">
							$$\begin{split}
							s_{\alpha}(i,j) &= (\mathcal{R_i}q_{i,\alpha})^{\top}(\mathcal{R_j}k_{j,\alpha}) \\
							                &= q_{i,\alpha}^{\top} \mathcal{R_i^{\top}}\mathcal{R_j}k_{j,\alpha} \\
											&= q_{i,\alpha}^{\top} \mathcal{R_{j-i}}k_{j,\alpha}
							\end{split}$$
						</p>
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 旋转位置编码

						$$s_{\alpha}(i,j) = q_{i,\alpha}^{\top} \mathcal{R_{j-i}}k_{j,\alpha}$$
						<p class="align-left">也就是说, 给:</p>
						<p class="align-left">位置为$m$的向量$q$乘上矩阵$\mathcal{R_m}$<br/>位置为$n$的向量$k$乘上矩阵$\mathcal{R_n}$</p>
						<p class="align-left">用变换后的$Q$, $K$序列做Attention</p>
						<p class="align-left">那么Attention就自动包含相对位置信息了</p>

						https://spaces.ac.cn/archives/8265
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						```python
						# RoPE (原作者)
						def apply_rotary_position_embeddings(sinusoidal, *tensors):
							"""应用RoPE到tensors中
							其中，sinusoidal.shape=[b, n, d]，tensors为tensor的列表，而
							tensor.shape=[b, n, ..., d]。
							"""
							assert len(tensors) > 0, 'at least one input tensor'
							assert all([
							    K.int_shape(tensor) == K.int_shape(tensors[0]) for tensor in tensors[1:]
							]), 'all tensors must have the same shape'
							ndim = K.ndim(tensors[0])
							sinusoidal = align(sinusoidal, [0, 1, -1], ndim)
							cos_pos = K.repeat_elements(sinusoidal[..., 1::2], 2, -1)
							sin_pos = K.repeat_elements(sinusoidal[..., ::2], 2, -1)
							outputs = []
							for tensor in tensors:
							    tensor2 = K.stack([-tensor[..., 1::2], tensor[..., ::2]], ndim)
							    tensor2 = K.reshape(tensor2, K.shape(tensor))
							    outputs.append(tensor * cos_pos + tensor2 * sin_pos)
							return outputs[0] if len(outputs) == 1 else outputs
						```

						[bert4keras:apply_rotary_position_embeddings()](https://github.com/bojone/bert4keras/blob/0cd0a4a0466f244ecc89bed0218dfeaa80fc2c35/bert4keras/backend.py#L316)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						```python
						# RoPE (pytorch)
						class Rotary(torch.nn.Module):
						    def __init__(self, dim, base=10000):
						        super().__init__()
						        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
						        self.register_buffer("inv_freq", inv_freq)
						        self.seq_len_cached = None
						        self.cos_cached = None
						        self.sin_cached = None

						    def forward(self, x, seq_dim=1):
						        seq_len = x.shape[seq_dim]
						        if seq_len != self.seq_len_cached:
						            self.seq_len_cached = seq_len
						            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)
						            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
						            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
						            self.cos_cached = emb.cos()[:, None, None, :]
						            self.sin_cached = emb.sin()[:, None, None, :]
						        return self.cos_cached, self.sin_cached

						def rotate_half(x):
						    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]
						    return torch.cat(
						        (-x2, x1), dim=x1.ndim - 1
						    )  # dim=-1 triggers a bug in torch < 1.8.0

						@torch.jit.script
						def apply_rotary_pos_emb(q, k, cos, sin):
						    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
						```
					</textarea>
				</section>
			</section>
			<section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 损失函数

						识别特定实体类型α $\Rightarrow$ 共有$\frac{n(n+1)}2$类的多标签分类问题
						<p class="fragment">最朴素的思路是变成$\frac{n(n+1)}2$个二分类</p>
						<p class="fragment">n=40 $\Rightarrow$ 820个二分类</p>
						<p class="fragment">每个句子的实体数很少 $\Rightarrow$ 不均衡</p>
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 损失函数

						$$\log \left(1 + \sum\limits_{i\in\Omega_{neg}} e^{s_i}\right) + \log \left(1 + \sum\limits_{j\in\Omega_{pos}} e^{-s_j}\right)$$
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 损失函数

						$$\log \left(1 + \sum\limits_{(i,j)\in P_{\alpha}} e^{-s_{\alpha}(i,j)}\right) + \log \left(1 + \sum\limits_{(i,j)\in Q_{\alpha}} e^{s_{\alpha}(i,j)}\right)$$
						`$$\begin{aligned}
							\Omega=&\,\big\{(i,j)\,\big|\,1\leq i\leq j\leq n\big\}\\
							P_{\alpha}=&\,\big\{(i,j)\,\big|\,t_{[i:j]}\text{是类型为}\alpha\text{的实体}\big\}\\
							Q_{\alpha}=&\,\Omega - P_{\alpha}
							\end{aligned}$$`
					</textarea>
				</section>
				<section data-markdown data-auto-animate>
					<textarea data-template>
						## 损失函数

						```python
						def multilabel_categorical_crossentropy(y_true, y_pred):
							"""y_pred 不用加激活函数, 尤其是不能加sigmoid或者softmax!"""
							y_pred = (1 - 2 * y_true) * y_pred
							y_pred_neg = y_pred - y_true * 1e12
							y_pred_pos = y_pred - (1 - y_true) * 1e12
							zeros = K.zeros_like(y_pred[..., :1])
							y_pred_neg = K.concatenate([y_pred_neg, zeros], axis=-1)
							y_pred_pos = K.concatenate([y_pred_pos, zeros], axis=-1)
							neg_loss = K.logsumexp(y_pred_neg, axis=-1)
							pos_loss = K.logsumexp(y_pred_pos, axis=-1)
							return neg_loss + pos_loss
						```

						https://spaces.ac.cn/archives/7359
					</textarea>
				</section>
			</section>
			<section>
				<section data-markdown>
					<textarea data-template>
						```python
						# GlobalPointer (原作者)
						class GlobalPointer(Layer):
							def __init__(
							    self,
							    heads,
							    head_size,
							    RoPE=True,
							    use_bias=True,
							    tril_mask=True,
							    kernel_initializer='lecun_normal',
							    **kwargs
							):
							    super(GlobalPointer, self).__init__(**kwargs)
							    self.heads = heads
							    self.head_size = head_size
							    self.RoPE = RoPE
							    self.use_bias = use_bias
							    self.tril_mask = tril_mask
							    self.kernel_initializer = initializers.get(kernel_initializer)

							def build(self, input_shape):
							    super(GlobalPointer, self).build(input_shape)
							    self.dense = Dense(
							        units=self.head_size * self.heads * 2,
							        use_bias=self.use_bias,
							        kernel_initializer=self.kernel_initializer
							    )

							def compute_mask(self, inputs, mask=None):
							    return None

							@recompute_grad
							def call(self, inputs, mask=None):
							    # 输入变换
							    inputs = self.dense(inputs)
							    inputs = tf.split(inputs, self.heads, axis=-1)
							    inputs = K.stack(inputs, axis=-2)
							    qw, kw = inputs[..., :self.head_size], inputs[..., self.head_size:]
							    # RoPE编码
							    if self.RoPE:
							        pos = SinusoidalPositionEmbedding(self.head_size, 'zero')(inputs)
							        qw, kw = apply_rotary_position_embeddings(pos, qw, kw)
							    # 计算内积
							    logits = tf.einsum('bmhd,bnhd->bhmn', qw, kw)
							    # 排除padding
							    logits = sequence_masking(logits, mask, '-inf', 2)
							    logits = sequence_masking(logits, mask, '-inf', 3)
							    # 排除下三角
							    if self.tril_mask:
							        mask = tf.linalg.band_part(K.ones_like(logits), 0, -1)
							        logits = logits - (1 - mask) * K.infinity()
							    # scale返回
							    return logits / self.head_size**0.5

							def compute_output_shape(self, input_shape):
							    return (input_shape[0], self.heads, input_shape[1], input_shape[1])

							def get_config(self):
							    config = {
							        'heads': self.heads,
							        'head_size': self.head_size,
							        'RoPE': self.RoPE,
							        'use_bias': self.use_bias,
							        'tril_mask': self.tril_mask,
							        'kernel_initializer':
							            initializers.serialize(self.kernel_initializer),
							    }
							    base_config = super(GlobalPointer, self).get_config()
							    return dict(list(base_config.items()) + list(config.items()))
						```

						[bert4keras: GlobalPointer](https://github.com/bojone/bert4keras/blob/master/bert4keras/layers.py#L1388)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						```python
						# GlobalPointer (pytorch)
						class GlobalPointer(nn.Module):
							def __init__(self, encoder, ent_type_size, inner_dim, RoPE=True):
							    super().__init__()
							    self.encoder = encoder
							    self.ent_type_size = ent_type_size
							    self.inner_dim = inner_dim
							    self.hidden_size = encoder.config.hidden_size
							    self.dense = nn.Linear(self.hidden_size, self.ent_type_size * self.inner_dim * 2)
							    self.RoPE = RoPE

							def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):
							    position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)

							    indices = torch.arange(0, output_dim // 2, dtype=torch.float)
							    indices = torch.pow(10000, -2 * indices / output_dim)
							    embeddings = position_ids * indices
							    embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)
							    embeddings = embeddings.repeat((batch_size, *([1]*len(embeddings.shape))))
							    embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))
							    embeddings = embeddings.to(self.device)
							    return embeddings

							def forward(self, input_ids, attention_mask, token_type_ids):
							    self.device = input_ids.device

							    context_outputs = self.encoder(input_ids, attention_mask, token_type_ids)
							    # last_hidden_state:(batch_size, seq_len, hidden_size)
							    last_hidden_state = context_outputs[0]

							    batch_size = last_hidden_state.size()[0]
							    seq_len = last_hidden_state.size()[1]

							    # outputs:(batch_size, seq_len, ent_type_size*inner_dim*2)
							    outputs = self.dense(last_hidden_state)
							    outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)
							    # outputs:(batch_size, seq_len, ent_type_size, inner_dim*2)
							    outputs = torch.stack(outputs, dim=-2)
							    # qw,kw:(batch_size, seq_len, ent_type_size, inner_dim)
							    qw, kw = outputs[...,:self.inner_dim], outputs[...,self.inner_dim:] # TODO:修改为Linear获取？

							    if self.RoPE:
							        # pos_emb:(batch_size, seq_len, inner_dim)
							        pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)
							        # cos_pos,sin_pos: (batch_size, seq_len, 1, inner_dim)
							        cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)
							        sin_pos = pos_emb[..., None,::2].repeat_interleave(2, dim=-1)
							        qw2 = torch.stack([-qw[..., 1::2], qw[...,::2]], -1)
							        qw2 = qw2.reshape(qw.shape)
							        qw = qw * cos_pos + qw2 * sin_pos
							        kw2 = torch.stack([-kw[..., 1::2], kw[...,::2]], -1)
							        kw2 = kw2.reshape(kw.shape)
							        kw = kw * cos_pos + kw2 * sin_pos

							    # logits:(batch_size, ent_type_size, seq_len, seq_len)
							    logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)

							    # padding mask
							    pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)
							    # pad_mask_h = attention_mask.unsqueeze(1).unsqueeze(-1).expand(batch_size, self.ent_type_size, seq_len, seq_len)
							    # pad_mask = pad_mask_v&pad_mask_h
							    logits = logits*pad_mask - (1-pad_mask)*1e12

							    # 排除下三角
							    mask = torch.tril(torch.ones_like(logits), -1)
							    logits = logits - mask * 1e12

							    return logits/self.inner_dim**0.5
						```
					</textarea>
				</section>
			</section>
			<section data-markdown>
				<textarea data-template>
					<div class="report">
					$$\begin{array}{c}
						\text{人民日报NER实验结果} \\
						{
							\begin{array}{c|cc|cc}
							\hline
							& \text{验证集F1} & \text{测试集F1} & \text{训练速度} & \text{预测速度}\\
							\hline
							\text{CRF} & 96.39\% & 95.46\% & 1\text{x} & 1\text{x}\\
							\text{GlobalPointer (w/o RoPE)} & 54.35\% & 62.59\% & 1.61\text{x} & 1.13\text{x} \\
							\text{GlobalPointer (w/ RoPE)}& 96.25\% & 95.51\% & 1.56\text{x} & 1.11\text{x} \\
							\hline
							\end{array}
						}
					\end{array}$$
					</div>
					<div class="report">
					$$\begin{array}{c}
						\text{CLUENER实验结果} \\
						{
							\begin{array}{c|cc|cc}
							\hline
							& \text{验证集F1} & \text{测试集F1} & \text{训练速度} & \text{预测速度}\\
							\hline
							\text{CRF} & 79.51\% & 78.70\% & 1\text{x} & 1\text{x}\\
							\text{GlobalPointer}& 80.03\% & 79.44\% & 1.22\text{x} & 1\text{x} \\
							\hline
							\end{array}
						}
					\end{array}$$
					</div>
					<div class="report">
					$$\begin{array}{c}
						\text{CMeEE实验结果} \\
						{
							\begin{array}{c|cc|cc}
							\hline
							& \text{验证集F1} & \text{测试集F1} & \text{训练速度} & \text{预测速度}\\
							\hline
							\text{CRF} & 63.81\% & 64.39\% & 1\text{x} & 1\text{x}\\
							\text{GlobalPointer}& 64.84\% & 65.98\% & 1.52\text{x} & 1.13\text{x} \\
							\hline
							\end{array}
						}
					\end{array}$$
					</div>
				</textarea>
			</section>
			<section data-markdown>
				<textarea data-template>
					# ```¯\_(ツ)_/¯```
				</textarea>
			</section>
		</div>
	</div>

	<script src="static/reveal.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/math/math.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/chalkboard/plugin.js"></script>

	<script>
		Reveal.initialize({
			chalkboard: { // font-awesome.min.css must be available
				toggleChalkboardButton: { left: "80px" },
				toggleNotesButton: { left: "130px" },
				colorButtons: 5
			},
			plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX, RevealChalkboard ]
		});
	</script>
</body>

</html>
